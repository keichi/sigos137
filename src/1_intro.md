# はじめに

<!-- 従来の静的な相互結合網の限界 -->
近年，HPC（High Performance Computing）システムを構成する計算ノード数の増加が
顕著である．HPCシステムにおける計算ノードは，相互結合網と呼ばれる高性能ネット
ワークにより互いに接続されている．今日の相互結合網は，
任意の計算ノード間で可能な限り有効帯域幅を最大化，かつ遅延を最小化す
るように，十分な余裕を持って設計される．しかし，このような十分に余裕を持たせた
設計方針では，ノード数のスケールアウトに伴い，相互結合網のトポロジは大規模化か
つ複雑化することが予測される．大規模かつ複雑なトポロジを有する相互結合網は，そ
れを構成するために必要となるスイッチやリンクの数が膨大となるため，構築コストの
観点から今後ますます実現が困難になっていくものと考えられる．

<!-- SDN-MPIの目的 -->
そのような背景から，われわれは，実際のネットワーク構成およびトラフィック状況等
に応じて相互結合網内で発生するノード間通信を動的に制御することで，HPCシステム
の相互結合網の利用率を高めることを目的とした研究開発を推進してきた．技術的に
は，大規模HPCシステム上で動作する並列分散アプリケーションの通信性能を向上させ
ることを目的とし，ネットワーク内のパケットフローを動的に制御可能とするSDNを，
並列分散プロセス間通信ライブラリMPIに応用したSDN-MPIを提案・実装してきた
\cite{Dashdavaa2013,Takahashi2014}．
`MPI_Bcast`\cite{Dashdavaa2013}では，MPIの集団通信の一つである`MPI_Bcast`をター
ゲットとし，HPCシステムの相互結合網のトポロジおよび実行時のプロセス配置情報を
もとに，最適なブロードキャスト配信を行う手法を実証した．また，
`MPI_Allreduce`\cite{Takahashi2014}では，冗長経路を含むトポロジに着目し，相互
結合網内のトラフィックに応じてリンクの負荷分散を行うことで，リンクにおける輻輳
を回避する手法を提案・実装した．

<!-- 問題 -->
これまでSDN-MPIのプロトタイプ実装により，個別のMPI関数を単独で実行した際の通信
時間の短縮を実証することができた．これらの手法・実装は，MPIの個別の集団通信の
高速化を実現するに留まっており，複数の集団通信が駆使された実際のMPIアプリケー
ションへの応用のためには，アプリケーションが呼び出すMPI関数と相互結合網の制御
を連動・連携して行う仕組みが必要不可欠である．本稿では，そのような問題点に着眼
し，われわれが開発してきたMPI通信パターンに基づくSDN制御を高速化するカーネルモ
ジュールについて報告する．
