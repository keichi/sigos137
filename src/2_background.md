# 技術課題

本節では，われわれが提案するSDN-MPIを概説し，本稿で取り扱う技術課題について記
す．そのために，まずSDN-MPIの構成技術であるSDN（Software-Defined Networking）
とMPI（Message Passing Interface）について説明する．

## SDN（Software-Defined Networking）

<!-- SDNの説明 -->
SDN（Software-Defined Networking）\cite{sdn}はパケットの流れを動的に集中制御可
能にするネットワークアーキテクチャである．従来のネットワークアーキテクチャでは
，パケット制御の決定を担うコントロールプレーンと実際のパケット転送を担うデータ
プレーンは，単一のネットワーク機器上に一体の機能となって実装されていた．SDNで
は，図\ref{fig:sdn-arch}に示すように，コントロールプレーンとデータプレーンを
それぞれ別々の機器に分離する．SDNスイッチと呼ばれるネットワーク機器はデータプ
レーンの処理，SDNコントローラはネットワークのコントロールプレーンの処理をソフ
トウェアにより担当する．この機能分離により，ネットワークがプログラム可能になる
こと，動的な制御ができること，集中制御ができることといった利点が実現される．

![SDNのアーキテクチャ\label{fig:sdn-arch}](sdn-arch.pdf)

SDNでは，パケットの流れの制御を「フロー」という単位で行う．各SDNスイッチは，フ
ローの集合である「フローテーブル」を保持する．表\ref{tbl:flow-table}にスイッチ
が保持するフローテーブルの一例を示す．フローは，対象のパケットを限定するヘッダ
フィールドと，パケットに対して行う操作を定義するアクションから成る．ヘッダ
フィールドの例として，TCP/UDPポート番号，IPアドレス，MACアドレスなどがある．一
方，アクションにはパケットを特定のポートへ転送する，パケットのヘッダを書き換え
る，パケットをドロップするなどの操作がある．

\begin{table}[htb]
    \centering
    \caption{フローテーブルの一例}
    \medskip
    \label{tbl:flow-table}
    \begin{tabular}{lllll}
        \hline
        \multicolumn{3}{c}{ヘッダフィールド}        &          & アクション \\ \hline
        宛先MAC           & 送信元IP   & 宛先IP     & $\cdots$ &            \\ \hline
                          & 192.0.2.12 & 192.0.2.34 &          & ポート1    \\
                          & 192.0.2.34 & 192.0.2.56 &          & ポート2    \\
        ff:ff:ff:ff:ff:ff &            &            &          & ポート1,2  \\
        72:42:c1:e4:75:8c &            &            &          & ドロップ
    \end{tabular}
\end{table}

SDNスイッチはパケットを受信すると，マッチするフローをフローテーブルから検索し
，フローのアクションを実行する．SDNコントローラは，ネットワーク内の全てのSDNス
イッチのフローテーブルを操作することで，ネットワーク内のパケットの流れを制御す
る．今日では，SDNの一実装として，OpenFlow\cite{McKeown2008}が業界標準規格とな
りつつある．

## MPI（Message Passing Interface）

MPI（Message Passing Interface）
\cite{Gropp1999,MessagePassingInterfaceForum2015}は，並列分散アプリケーション
の開発に用いられるプロセス間通信ライブラリである．MPIはメッセージパッシングと
いうプログラミングモデルに基いている．メッセージパッシングでは，並列分散プロ
セス同士は，メッセージと呼ばれるデータの交換により協調して動作する．MPIは，相
互結合網を構成するハードウェアや通信プロトコルの差異を吸収するため，アプリケー
ションの開発効率と可搬性を向上する．今日では，HPCシステムで動作するアプリケー
ションの多くがMPIを利用しているため，MPIの通信性能を改善することにより，アプリ
ケーションの実行時間の短縮を期待できる．

MPIにおいて定義されている関数は，1対1通信関数と集団通信関数に分類される．1対1
通信関数は，2つのプロセス間で通信する関数であり，`MPI_Send`や`MPI_Recv`などが
代表的な例としてあげられる．一方，集団通信関数は複数のプロセスから成る
グループの中で通信を実施する関数である．集団通信関数の例として，1つのプロセス
から複数のプロセスに同一のデータを配信する`MPI_Bcast`や，複数のプロセスから1つ
のプロセスにデータを縮約する`MPI_Reduce`などがある．

これらのMPI通信関数は引数として，通信に参加するプロセスを指定する．1対1通信の
場合は送信元プロセス・宛先プロセスの識別子，集団通信の場合は通信に参加するプロ
セスのグループの識別子を，それぞれ引数として渡す．MPI関数の種類と，これらの参
加プロセスの情報を合わせて，本報告では通信パターンと定義する．

## SDN-MPI実現のための技術課題

<!-- SDN-MPIのコンセプト -->
われわれは，\ref{sdnsoftware-defined-networking}節で述べたSDNによるフローの動
的制御機能に着眼し，並列分散プロセス間通信ライブラリMPIの通信性能向上に応用する
，SDN-MPIの研究開発を推進してきた。図 \ref{fig:sdn-mpi}にSDN-MPIのコンセプトを
示す．SDN-MPIでは，MPIを利用するアプリケーションから通信パターンを抽出し，SDN
を用いて，通信性能を向上するために最適なフローの制御を実現することを目的とする
．われわれは，これまでの研究開発により，SDN-MPIのアイディアに基いて，
`MPI_Bcast`関数と`MPI_Allreduce`関数の高速化に成功した
\cite{Dashdavaa2013,Takahashi2014}．

![SDN-MPI概念図\label{fig:sdn-mpi}](sdn-mpi.pdf)

しかし，これまでの研究では，個別のMPI関数を単独で実行することに主眼があり，未
だ実際のアプリケーションを稼働させるに至っていない．その理由は，アプリケーショ
ンの実行とSDNによる相互結合網内のフローの制御を同期する仕組みが存在しなかった
ことにある．実際のMPIアプリケーションでは，単一のアプリケーションが複数のMPI関
数を実行する．そのため，アプリケーションがMPI関数を呼び出すごとに，その通信パ
ターンをリアルタイムに把握する必要がある．そして，通信パターンに応じて，通信性
能を向上するように相互結合網内のフローの制御を行わなければいけない．この同期手
法の実現にあたり，以下の技術課題がある：

- オーバーヘッドの削減：アプリケーションの実行とフローの制御は，可能な限り小さ
    いオーバーヘッドで同期しなければいけない．
- スケーラビリティの向上：SDNスイッチにインストールできるフロー数の上限をでき
    るだけ増やすことができる方法が望ましい．
- MPIに由来するパケットの識別：相互結合網内にはSSHなど，MPIと無関係なパケット
    も流れる．そのため，MPIが送出したパケットのみを判別し，制御する必要がある
    ．

